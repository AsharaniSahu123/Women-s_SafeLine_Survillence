{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a452f22-3fbe-4f6d-9f4a-b294730c3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without open AI\n",
    "import cv2\n",
    "import numpy as np\n",
    "import paho.mqtt.client as mqtt\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "# MQTT Configuration\n",
    "MQTT_BROKER = \"mqtt.eclipseprojects.io\"\n",
    "MQTT_PORT = 1883\n",
    "MQTT_TOPIC = \"road/safety/alerts\"\n",
    "\n",
    "# Function to publish MQTT messages\n",
    "def publish_alert(message):\n",
    "    client = mqtt.Client()\n",
    "    client.connect(MQTT_BROKER, MQTT_PORT, 60)\n",
    "    client.publish(MQTT_TOPIC, message)\n",
    "    client.disconnect()\n",
    "\n",
    "# Function to show alert box\n",
    "def show_alert(message):\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide main window\n",
    "    messagebox.showwarning(\"Threat Alert!\", message)\n",
    "    root.destroy()\n",
    "\n",
    "# Load YOLO model for person and threat detection\n",
    "yolo_net = cv2.dnn.readNet(\"yolo-coco/yolov4.weights\", \"yolo-coco/yolov4.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"yolo-coco/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load gender classification model\n",
    "gender_net = cv2.dnn.readNetFromCaffe(\n",
    "    \"gender-class/gender_deploy.prototxt\", \"gender-class/gender_net.caffemodel\"\n",
    ")\n",
    "gender_list = [\"Male\", \"Female\"]\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"Real-time Detection\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    detections = yolo_net.forward(output_layers)\n",
    "\n",
    "    boxes, confidences, class_ids = [], [], []\n",
    "    male_count, female_count = 0, 0\n",
    "    threats_detected = []\n",
    "\n",
    "    for detection in detections:\n",
    "        for object_detected in detection:\n",
    "            scores = object_detected[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.4:\n",
    "                center_x, center_y = int(object_detected[0] * width), int(object_detected[1] * height)\n",
    "                w, h = int(object_detected[2] * width), int(object_detected[3] * height)\n",
    "                x, y = int(center_x - w / 2), int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            class_id, x, y, w, h = class_ids[i], *boxes[i]\n",
    "            label = classes[class_id]\n",
    "            roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "            if label == \"person\" and roi.size > 0:\n",
    "                blob_gender = cv2.dnn.blobFromImage(roi, 1, (227, 227), (78.426337, 87.768914, 114.895847), swapRB=False)\n",
    "                gender_net.setInput(blob_gender)\n",
    "                gender_preds = gender_net.forward()\n",
    "                gender = gender_list[np.argmax(gender_preds)]\n",
    "\n",
    "                if gender == \"Male\":\n",
    "                    male_count += 1\n",
    "                    color = (255, 0, 0)  # Blue for male\n",
    "                else:\n",
    "                    female_count += 1\n",
    "                    color = (0, 255, 0)  # Green for female\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, gender, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            if label in [\"knife\", \"gun\", \"scissors\"]:\n",
    "                threats_detected.append(label)\n",
    "                color = (0, 0, 255)  # Red for threats\n",
    "                \n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"Threat: {label}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "                \n",
    "                publish_alert(f\"ALERT: Threat detected - {label}\")\n",
    "                show_alert(f\"Threat detected\")\n",
    "\n",
    "    cv2.putText(frame, f\"Male: {male_count}, Female: {female_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    if male_count > female_count and female_count > 0:\n",
    "        publish_alert(\"RED ALERT: Unbalanced male-to-female ratio detected!\")\n",
    "        show_alert(\"RED ALERT: Unbalanced male-to-female ratio detected!\")\n",
    "\n",
    "    cv2.imshow(\"Real-time Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165e01bb-c31b-4b3e-a80a-fa6c9acfd359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\ashar\\anaconda3\\lib\\site-packages (1.79.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e6d03f5-d241-4625-9e26-3b61f800b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# OpenAI API Key (Replace with your own key)\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "# Function to generate AI-based explanation\n",
    "def generate_threat_explanation(threat_label):\n",
    "    prompt = f\"A {threat_label} has been detected in a public area. Explain the possible risks and recommended safety measures.\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant providing security guidance.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Modify the existing threat detection logic\n",
    "if label in [\"knife\", \"gun\", \"scissors\"]:\n",
    "    threats_detected.append(label)\n",
    "    color = (0, 0, 255)  # Red for threats\n",
    "    \n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "    cv2.putText(frame, f\"Threat: {label}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    ai_explanation = generate_threat_explanation(label)\n",
    "    \n",
    "    publish_alert(f\"ALERT: Threat detected - {label}\\n{ai_explanation}\")\n",
    "    show_alert(f\"Threat detected: {label}\\n{ai_explanation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07483e94-9468-4e86-814d-db438c38dbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\ashar\\anaconda3\\lib\\site-packages (1.68.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashar\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9dc0b-8664-42a1-803d-44e7edcaae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashar\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c9bf7-27cb-4326-9925-c8be982d3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using openAI for Desktop\n",
    "import cv2\n",
    "import numpy as np\n",
    "import paho.mqtt.client as mqtt\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import openai\n",
    "\n",
    "# OpenAI API Client (Latest Version)\n",
    "client = openai.OpenAI(api_key=\"your_openai_api_key\")\n",
    "\n",
    "# MQTT Configuration\n",
    "MQTT_BROKER = \"mqtt.eclipseprojects.io\"\n",
    "MQTT_PORT = 1883\n",
    "MQTT_TOPIC = \"road/safety/alerts\"\n",
    "\n",
    "# Function to publish MQTT messages\n",
    "def publish_alert(message):\n",
    "    client = mqtt.Client()\n",
    "    client.connect(MQTT_BROKER, MQTT_PORT, 60)\n",
    "    client.publish(MQTT_TOPIC, message)\n",
    "    client.disconnect()\n",
    "\n",
    "# Function to show alert box\n",
    "def show_alert(message):\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide main window\n",
    "    messagebox.showwarning(\"Threat Alert!\", message)\n",
    "    root.destroy()\n",
    "\n",
    "# Function to analyze threats using OpenAI GPT\n",
    "def analyze_threat(threats_detected):\n",
    "    threat_text = f\"Detected threats: {', '.join(threats_detected)}. Suggest a safety response.\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a safety advisor.\"},\n",
    "            {\"role\": \"user\", \"content\": threat_text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content  # Updated return format\n",
    "\n",
    "# Load YOLO model for object detection\n",
    "yolo_net = cv2.dnn.readNet(\"yolo-coco/yolov4.weights\", \"yolo-coco/yolov4.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"yolo-coco/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load gender classification model\n",
    "gender_net = cv2.dnn.readNetFromCaffe(\n",
    "    \"gender-class/gender_deploy.prototxt\", \"gender-class/gender_net.caffemodel\"\n",
    ")\n",
    "gender_list = [\"Male\", \"Female\"]\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"Real-time Detection\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    detections = yolo_net.forward(output_layers)\n",
    "\n",
    "    boxes, confidences, class_ids = [], [], []\n",
    "    male_count, female_count = 0, 0\n",
    "    threats_detected = []\n",
    "\n",
    "    for detection in detections:\n",
    "        for object_detected in detection:\n",
    "            scores = object_detected[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.4:\n",
    "                center_x, center_y = int(object_detected[0] * width), int(object_detected[1] * height)\n",
    "                w, h = int(object_detected[2] * width), int(object_detected[3] * height)\n",
    "                x, y = int(center_x - w / 2), int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            class_id, x, y, w, h = class_ids[i], *boxes[i]\n",
    "            label = classes[class_id]  # Ensure label is assigned\n",
    "\n",
    "            roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "            # Gender classification\n",
    "            if label == \"person\" and roi.size > 0:\n",
    "                blob_gender = cv2.dnn.blobFromImage(roi, 1, (227, 227), \n",
    "                                                    (78.426337, 87.768914, 114.895847), swapRB=False)\n",
    "                gender_net.setInput(blob_gender)\n",
    "                gender_preds = gender_net.forward()\n",
    "                gender = gender_list[np.argmax(gender_preds)]\n",
    "\n",
    "                color = (255, 0, 0) if gender == \"Male\" else (0, 255, 0)\n",
    "                if gender == \"Male\":\n",
    "                    male_count += 1\n",
    "                else:\n",
    "                    female_count += 1\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, gender, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            # Threat detection\n",
    "            if label in [\"knife\", \"gun\", \"scissors\"]:\n",
    "                threats_detected.append(label)\n",
    "                color = (0, 0, 255)  # Red for threats\n",
    "                \n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"Threat: {label}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Display male and female counts\n",
    "    cv2.putText(frame, f\"Male: {male_count}, Female: {female_count}\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Alert if unbalanced gender ratio\n",
    "    if male_count > female_count:\n",
    "        alert_msg = \"RED ALERT: Unbalanced male-to-female ratio detected!\"\n",
    "        publish_alert(alert_msg)\n",
    "        show_alert(alert_msg)\n",
    "\n",
    "    # AI-based threat analysis\n",
    "    if threats_detected:\n",
    "        ai_advice = analyze_threat(threats_detected)\n",
    "        publish_alert(f\"ALERT: {ai_advice}\")\n",
    "        show_alert(ai_advice)\n",
    "\n",
    "    # Show video frame\n",
    "    cv2.imshow(\"Real-time Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb1e70-abd6-4427-8a65-f2f188ba85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using openAI for Android\n",
    "import cv2\n",
    "import numpy as np\n",
    "import paho.mqtt.client as mqtt\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import openai\n",
    "\n",
    "# OpenAI API Key (Replace with your actual key)\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "# MQTT Configuration\n",
    "MQTT_BROKER = \"mqtt.eclipseprojects.io\"\n",
    "MQTT_PORT = 1883\n",
    "MQTT_TOPIC = \"road/safety/alerts\"\n",
    "\n",
    "# Android camera URL (replace with your phone's IP)\n",
    "ANDROID_CAM_URL = \"http://192.0.0.4:8080/video\"\n",
    "\n",
    "# Function to publish MQTT messages\n",
    "def publish_alert(message):\n",
    "    client = mqtt.Client()\n",
    "    client.connect(MQTT_BROKER, MQTT_PORT, 60)\n",
    "    client.publish(MQTT_TOPIC, message)\n",
    "    client.disconnect()\n",
    "\n",
    "# Function to show alert box\n",
    "def show_alert(message):\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    messagebox.showwarning(\"Threat Alert!\", message)\n",
    "    root.destroy()\n",
    "\n",
    "# Function to analyze threats using OpenAI GPT\n",
    "def analyze_threat(threats_detected):\n",
    "    threat_text = f\"Detected threats: {', '.join(threats_detected)}. Suggest a safety response.\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a safety advisor.\"},\n",
    "                  {\"role\": \"user\", \"content\": threat_text}]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_net = cv2.dnn.readNet(\"yolo-coco/yolov4.weights\", \"yolo-coco/yolov4.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"yolo-coco/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Connect to Android camera\n",
    "cap = cv2.VideoCapture(ANDROID_CAM_URL)\n",
    "cv2.namedWindow(\"Real-time Detection\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to retrieve frame. Check the camera stream URL.\")\n",
    "        break\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    detections = yolo_net.forward(output_layers)\n",
    "\n",
    "    boxes, confidences, class_ids = [], [], []\n",
    "    threats_detected = []\n",
    "\n",
    "    for detection in detections:\n",
    "        for obj in detection:\n",
    "            scores = obj[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.4:\n",
    "                center_x, center_y = int(obj[0] * width), int(obj[1] * height)\n",
    "                w, h = int(obj[2] * width), int(obj[3] * height)\n",
    "                x, y = int(center_x - w / 2), int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            class_id, x, y, w, h = class_ids[i], *boxes[i]\n",
    "            label = classes[class_id]\n",
    "\n",
    "            # Threat detection\n",
    "            if label in [\"knife\", \"gun\", \"scissors\"]:\n",
    "                threats_detected.append(label)\n",
    "                color = (0, 0, 255)  # Red for threats\n",
    "                \n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(frame, f\"Threat: {label}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    # AI-based threat analysis\n",
    "    if threats_detected:\n",
    "        ai_advice = analyze_threat(threats_detected)\n",
    "        publish_alert(f\"ALERT: {ai_advice}\")\n",
    "        show_alert(ai_advice)\n",
    "\n",
    "    # Show video frame\n",
    "    cv2.imshow(\"Real-time Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e928fad-956e-49ec-a3bd-5f6cd6170201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
